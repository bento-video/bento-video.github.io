<!DOCTYPE html>

<html lang="en-US" prefix="og: http://opg.me/ns#">

<head>
  <meta charset="UTF-8" />

  <meta name="title" property="og:title" content="Bento" />

  <meta name="description" property="og:description"
    content="Bento is a blazing fast serverless video transcoding pipeline." />

  <meta name="type" property="og:type" content="website" />

  <meta name="url" property="og:url" content="https://bento-video.github.io/" />

  <meta name="image" property="og:image" content="#" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="author" content="Mike del Rio, Max Hawkins, Nathan Classen" />

  <title>Bento Case Study</title>

  <link rel="icon" type="image/png" sizes="16x16" href="images/icons/favicon_package_v0.16/favicon-16x16.png" />
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,500&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Lato&display=swap" rel="stylesheet">

  <!-- <style>reset</style> -->

  <link rel="stylesheet" href="stylesheets/reset.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/gruvbox-dark.min.css"
    charset="utf-8" />

  <!-- <style></style> -->

  <link rel="stylesheet" href="stylesheets/main.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="javascripts/main.js"></script>

  <!-- <script></script> -->

  <script src="javascripts/application.js"></script>
</head>

<body>
  <div class="logo-links">
    <p id="bento-logo">
      <img src="./images/logos/bento_logo.png" />
    </p>
    <a href="https://github.com/orgs/bento-video/dashboard" target="_blank">
      <img src="images/github/github_black.png" alt="github logo" id="github-logo" />
    </a>
  </div>
  <a id="toTop-link" href="#" target="_blank">
    <img src="images/icons/toTop-icon.png" alt="Back to top" id="toTop-logo" />
  </a>
  <header id="home">
    <h1>
      <img src="images/logos/bento_logo.png" alt="Bento logo" />
      <p>a blazing fast video transcoding pipeline</p>
    </h1>
  </header>


  <section class="integration">
    <div class="integration-wrapper">
      <div class="box">
        <img src="#" alt="" />
      </div>

      <article class="box">
        <div class="text-box">
          <h1>High speed video transcoding in the cloud</h1>

          <p>
            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum convallis erat non mauris finibus
            iaculis.
          </p>

        </div>
      </article>
    </div>
  </section>

  <section class="integration">
    <div class="integration-wrapper">

      <article class="box">
        <div class="text-box">
          <h1>High speed video transcoding in the cloud.</h1>
          <p>
            Transcode your videos at lightning speed.
          </p>
        </div>
      </article>

      <div class="box">
        <img src="images/gifs/fast-transcode.gif" alt="" class="softened" />
      </div>
    </div>
  </section>

  <section class="integration">
    <div class="integration-wrapper">

      <div class="box" id="">
        <img src="#" alt="" />
      </div>

      <article class="box">
        <div class="text-box">
          <h1>Nunc a accumsan lorem.</h1>

          <p>
            Curabitur odio dolor, venenatis ac dolor at, malesuada placerat nunc. Donec non augue et mi rutrum rutrum
            non eu magna.
          </p>

        </div>
      </article>

    </div>
  </section>

  <main>
    <section id="case-study">
      <h1>Case Study</h1>

      <!-- <div id="side-nav">
        <img src="images/logos/bento_logo.png" alt="Bento Logo" />
      </div> -->
      <div id="side-nav">
        <img src="images/logos/bento_logo.png" alt="Bento Logo" id="nav-logo" />
      </div>
      <nav>
        <ul id="nav-links">
          <li id="introduction-nav">
            <a class="case-study-anchor" href="#introduction">introduction</a>
          </li>
          <li id="serverless-nav">
            <a class="case-study-anchor" href="#serverless">serverless</a>
          </li>
          <li id="building-nav">
            <a class="case-study-anchor" href="#building">building</a>
          </li>
          <li id="architecture-nav">
            <a class="case-study-anchor" href="#architecture">architecture</a>
          </li>
          <li id="challenges-nav">
            <a class="case-study-anchor" href="#challenges">challenges</a>
          </li>
          <li id="results-nav">
            <a class="case-study-anchor" href="#results">results</a>
          </li>
          <li id="future-work-nav">
            <a class="case-study-anchor" href="#future-work">future work</a>
          </li>
        </ul>
      </nav>

      <div class="chapter">
        <h2 id="introduction">1 Introduction</h2>

        <h3>1.1 What is Bento?</h3>

        <p>Bento is a blazing fast serverless video transcoding pipeline that can easily be deployed to Amazon Web
          Services (AWS). It is built for individuals and small businesses seeking a fast, simple, open-source solution
          to their video transcoding needs.</p>

        <p>Converting high-definition videos to new formats and resolutions is a process that can take hours on a single
          machine. Businesses that serve videos to their users typically turn to video transcoding services, which are
          expensive, or build their own video transcoding server farms, which are complex and error-prone.</p>

        <p>By leveraging the instant scale and concurrent execution of function-as-a-service architecture, Bento
          transcodes hundreds of small parts of a video file in parallel, slashing multi-hour transcoding jobs down to
          minutes.</p>

        <p>This case study will outline the approach that we took to developing a serverless solution to video
          transcoding, and will explore some of the technical challenges of processing large video files in a highly
          concurrent cloud system. We will conclude by benchmarking Bento against typical competitors to demonstrate the
          impressive results yielded by taking this approach to video transcoding.</p>

        <p>To begin, let’s take a step back and discuss what video transcoding is, and why video is among the most
          compelling challenges in web development. </p>

        <h3>1.2 Why is video interesting?</h3>

        <p>Video is ubiquitous on the web. Video-on-demand platforms are billion-dollar businesses. Video dominates
          social media feeds and internet ads. Yet for all of this, video is often taken for granted. We rarely stop to
          consider how unique and interesting it is.</p>

        <p>For one, video is ubiquitous. Last year, video made up around 76% of all internet traffic. With new
          video-based services and apps launching seemingly every day, video traffic is predicted to grow to over 80% of
          total internet traffic within the next few years.</p>

        <p>Video is also distinct from most other web traffic because video files tend to be very large; they are
          usually an order of magnitude larger than images. As a result, video tends to present challenges related to
          storage, memory, and bandwidth that other types of resources don’t present. In addition, the size of video
          files is increasing as advancing video technologies allow for higher resolutions and higher frame rates. As a
          result, these challenges are expected to grow.</p>

        <div class="img-wrapper">
          <img src="images/diagrams/filesize-scale.png" alt="text and code cells example" />
        </div>

        <p>Finally, video is complex. Delivering video files over the internet is more complicated than perhaps any
          other file type. The source of this complexity comes from two major challenges: the compatibility problem and
          the bandwidth problem. We will delve into these two problems as an introduction to video transcoding and why
          it is necessary.</p>

        <h3>1.3 What is video transcoding?</h3>

        <h4>1.3.1 What is a video file?</h4>

        <p>To discuss the problems surrounding compatibility and bandwidth, we must first explain what a video file is.
        </p>

        <p>When a video is first recorded by a phone or camera, the raw video data is too large to store locally, much
          less send over the web. For example, an hour of 1080p60fps uncompressed video is around 1.3 terabytes. </p>

        <p>To bring the size of this raw data down to a more manageable size, the data must be compressed. The software
          that is used to compress this data is called a codec (a combination of the words coder and decoder). A codec
          applies an algorithm to compress video data, encoding it so that it can be easily stored and sent. Once
          compressed, the data is packaged into a file format, called a container. Containers have extensions you may
          have seen, like .mp4 or .mov</p>

        <p>When playing a video this process is reversed. A media player opens the container, and the same codec is used
          to decode the video data and display it on the device. </p>

        <div class="img-wrapper">
          <img src="images/diagrams/playback-decoding.png" alt="text and code cells example" />
          <p>Encoded videos are decoded with the same codec for playback</p>
        </div>

        <h4>1.3.2 The problem of compatibility</h4>

        <p>The first problem that businesses face is that there are dozens of different codecs, containers, and video
          players, each with their own strengths and weaknesses. Unfortunately, they are not all compatible with each
          other. Using the wrong codec or container could mean that some users can’t play certain videos on their
          device. Therefore a decision must be made as to which codec and container will be used to package a video.</p>

        <div class="img-wrapper">
          <img src="images/diagrams/complex-compatibility.png" alt="text and code cells example" />
        </div>

        <p>Usually this decision will be made based on the characteristics of the codec and the types of devices or
          media players that a business expects their users to have. Once they have made this decision, they need to
          convert any video files they have using the codec and container they have decided on.</p>

        <div class="img-wrapper">
          <img src="images/diagrams/compatibility-solution.png" alt="text and code cells example" />
          <p>Businesses select a codec and container format to optimize compatibility</p>
        </div>

        <p>However, now they need to answer a second question: how much should they compress their videos?</p>

        <h4>1.3.3 The problem of bandwidth</h4>

        <p>Generally speaking, there is a negative correlation between the level of compression and the quality of the
          resultant video. The more a video file is compressed, the greater the decrease in visual fidelity. This means
          that the less a video is compressed, the more of its original quality is preserved and the larger the file
          size. However, not all users will have the bandwidth to quickly download larger, higher quality files.</p>

        <div class="img-wrapper">
          <img src="images/diagrams/compression-example.png" alt="text and code cells example" />
          <p>Higher levels of compression diminish video quality. <br> 1080p (top) vs 360p (bottom) </p>
        </div>

        <p>Consider for instance, the difference in download speed that will be available to a user on a fiber internet
          connection in their office, and a user on 3G mobile connection going through a subway tunnel as they both
          attempt to download the same video. The person in their office will have a smooth experience, whereas the
          person in the tunnel may have a choppy experience, if their video plays at all.</p>

        <p>For this reason, businesses will usually create multiple versions of the same video at different rates of
          compression, and thus different file sizes. Modern media players detect users’ bandwidth, and deliver the
          video file most appropriate for speed of their connection. Smaller, more compressed videos will be delivered
          to users with less available bandwidth, while users with stronger connections will be served higher quality
          videos.</p>

        <div class="img-wrapper">
          <img src="images/diagrams/variable-bandwidth.png" alt="text and code cells example" />
        </div>

        <h4>1.3.4 Video transcoding and it's own inherent difficulties</h4>

        <div class="img-wrapper">
          <img src="images/diagrams/transcoding-recap.png" alt="text and code cells example" />
        </div>

        <p>To recap, businesses that deliver video will use a codec to convert their videos into a single container
          format, compressed to multiple file sizes. This process of conversion is called video transcoding. The process
          of ingesting a video file and transcoding it to specific formats and file sizes is coordinated and facilitated
          by a transcoding pipeline.</p>

        <p>Every business that delivers video files on the internet will consider how they are going to handle this
          process of transcoding their videos. Unfortunately, the video transcoding process possesses its own inherent
          challenges that will need to be addressed when negotiating a transcoding solution.</p>

        <p>First, transcoding large video files takes a very long time. Transcoding a single 60 minute HD video can take
          anywhere from two to six hours and sometimes more. </p>

        <p>Second, In addition to costs in time, transcoding is demanding on memory and CPU. A video transcoding process
          will happily eat all the CPU that is thrown at it. Transcoding on a single local machine may be a viable
          option for individuals only transcoding one of two videos a month. </p>

        <p>However, individuals and businesses with regular video demand will not find this to be a feasible option and
          will instead utilize a custom in-house transcoding pipeline or a third-party transcoding service.</p>

        <h3>1.4 Existing video transcoding solutions</h3>

        <p>Broadly considered, professional video transcoding solutions will fall into two categories: customized
          solutions that are developed and maintained in-house and third party commercial software.</p>

        <h4>1.4.1 Custom transcoding</h4>

        <div class="img-wrapper">
          <img src="images/diagrams/solution1-tradeoffs.png" alt="text and code cells example" />
        </div>

        <p>Some businesses choose to build their own video transcoding farm in-house. In this case, a development team
          is needed to build custom transcoding pipeline and deploy it to bare metal servers or cloud infrastructure
          (e.g. Amazon EC2, Digital Ocean Droplet).
        </p>

        <p>Once deployed, this option is cheaper than third-party software, and it provides businesses with maximum
          control over what formats, codecs, and video settings they support, as well as any additional transformations
          they want to make to their videos.</p>

        <p>This option requires a high amount of technical expertise both to develop and scale effectively: video
          transcoding can be both slow and error prone without significant engineering attention. As we’ll see later,
          provisioning servers for video transcoding can also result in periods wherein compute power is going unused.
        </p>

        <p>Building a custom transcoding farm is best-suited for video platforms such as Netflix and YouTube. Companies
          that ingest and stream millions of hours of video each week can build their own engineering operations around
          this task to minimize the trade-offs.</p>

        <h4>1.4.2 Commercial software</h4>

        <div class="img-wrapper">
          <img src="images/diagrams/solution2-tradeoffs.png" alt="text and code cells example" />
        </div>

        <p>A second solution is to use commercial video transcoding software. Services such as Amazon MediaConvert and
          Zencoder offer powerful cloud-based transcoding.
        </p>

        <p>These services provide comprehensive solutions for businesses that need to support multiple formats, codecs,
          and devices. Because these services specialize in video, they will be tuned for speed and adept at handling
          the error cases that typically accompany transcoding jobs.</p>

        <p>However, the number of transcoding options many of these services provide may be overkill for smaller
          businesses that have fewer input and output requirements. And, as one might expect, these services tend to be
          a more expensive option.</p>

        <p>Transcoding services are a good fit for media production companies like (e.g. HBO, BBC) that produce massive
          amounts of video content that lands on many devices, and don’t want to build large software engineering teams
          around video transcoding.</p>

        <h4>1.4.3 Solutions compared</h4>

        <p>Let’s briefly review the options outlined above.</p>

        <div class="img-wrapper">
          <img src="images/diagrams/solutions-compared.png" alt="text and code cells example" />
        </div>

        <p>Custom pipelines provide the highest level of control over inputs and outputs. The bottleneck is technical
          expertise - to really get the best performance and cost from this model, a business needs to build a dedicated
          software engineering team around video transcoding.</p>

        <p>By contrast, businesses can outsource their video transcoding teams to third-party services. These services
          provide the benefits of speed and control, but at a higher cost.</p>


        <p>These options are sufficient for some businesses, but the Bento team saw an opportunity here. Is it possible
          to build a fast, low-cost transcoding pipeline, suited for businesses that don’t have video expertise and
          won’t need a plethora of video options? We felt that the answer was yes, and that the solution might lie in
          serverless technology.</p>
      </div>
      <div class="chapter">
        <h2 id="serverless"> 2 Toward a serverless Architecture</h2>

        <p>Bento is a “serverless” transcoding pipeline, but what does that mean? As it turns out, serverless is a bit
          of an overloaded term and it’s not always clear what exactly is being referred to.</p>

        <h3>2.1 What is serverless?</h3>

        <p>Serverless originally referred to applications that would outsource business-logic and state management to
          third party services. </p>

        <p>Here, a robust client application interacts with a cloud-hosted backend that is <em>implemented</em>
          by some third
          party. This kind of architecture is sometimes called Backend as a Service (<strong>BaaS</strong>). You, as the
          developer,
          create a client side application and the BaaS provides functionality like storage, caching, and authentication
          via an API.</p>

        <p>These days, however, serverless usually refers to a related but distinct architecture called <em>Function as
            a
            Service</em>, or <strong>FaaS</strong>.</p>

        <h3>2.2 Function as a Service</h3>

        <h4>2.2.1 What is Function as a Service?</h4>

        <p>Function as a Service (FaaS) is similar to BaaS in that both abstract away the details of infrastructure
          scaling and management.</p>

        <p>However, in contrast to BaaS, developers are responsible for implementing their backend code. The most
          significant distinction comes from how this code is deployed and executed:</p>

        <p>In a FaaS architecture, application code is executed in short-lived compute containers that run in response
          to an event. </p>

        <p>Let’s dissect that statement. The key characteristics of Function as a Service are:</p>

        <ol>
          <li><strong>Code is executed in response to an event:</strong> An event may be an HTTP request from the
            client, or emitted from
            a server-side resource. </li>
          <li><strong>The code is run within ephemeral containers:</strong> These containers are spun up to execute code
            in response to
            an event, and immediately destroyed once all incoming events have been processed. This is a significant
            distinction from traditional web servers, which are intended to run persistently whether or not they are
            actively responding to incoming requests.</li>
          <li><strong>Containers automatically scale in response to incoming load:</strong> The number of containers
            that are executing
            code will increase or decrease to meet the load of incoming events.</li>
          <li><strong>Multiple instances of a function can execute concurrently:</strong> Because containers scale in
            response to
            incoming load, multiple instances of a function can execute concurrently.</li>
          <li><strong>FaaS containers are stateless:</strong> Data does not persist outside of function instances, and
            data cannot
            directly pass between functions executing concurrently in separate containers. </li>
        </ol>

        <p>As a consequence of these characteristics, FaaS architecture does not require long running servers or
          applications. A function’s container will only run for the time it takes the function to complete. Developers
          simply write code and upload it to a cloud server, knowing that their code will execute when a specific event
          occurs.
        </p>

        <p>Cloud computing services like Amazon Web Services, Google Cloud Platform, and Microsoft Azure offer their own
          FaaS platforms (AWS Lambda, Cloud Functions, and Azure Functions, respectively). These providers provision the
          necessary application environment and computing resources to run the provided code, as well as scaling the
          execution of the function as needed.</p>

        <h4>2.2.2 The benefits of FaaS</h4>

        <div class="img-wrapper">
          <img src="images/diagrams/faas-benefits.png" alt="text and code cells example" />
        </div>

        <p>Function as a Service provides a number of benefits over traditional long-lived server architectures.</p>

        <p>The first benefit is core to serverless computing as a whole, which is the delegation of infrastructure
          management to a third-party. In particular, FaaS enables developers to write and quickly deploy application
          code while concerns around resource provisioning, execution environment, and availability are delegated to the
          FaaS provider.</p>

        <p>Function as a service can be cost efficient for certain types of workflows, as developers are not charged for
          idle server time. Instead, billing is related to compute time which starts when a function is triggered by an
          event and ends when the function’s code execution is complete. Compute time can be calculated in increments as
          small as 100 milliseconds. This means that short-duration processes that run intermittently can see a
          significant reduction in cost relative to running on a persistent server.</p>

        <p>The most important features of FaaS in the context of Bento’s architecture are the automated scaling and
          concurrent execution of function containers. Combined, these two features enable us to execute hundreds of
          transcoding jobs in parallel, <em>within seconds</em>.</p>

        <p>These behaviors are really the bread and butter of Bento, and we will dive into them more deeply in a short
          while. Before we do that, let’s consider the tradeoffs of building with FaaS.</p>

        <h4>2.2.3 The limitations of Function as a Service</h4>

        <p>Function as a Service also presents drawbacks and challenges in relation to traditional server architecture.
        </p>

        <h5>Control</h5>

        <p>Function as a Service exists on a spectrum of infrastructure solutions that are intended to abstract away an
          increasing amount of administration, configuration, and maintenance responsibilities that exist alongside any
          scalable web application.</p>

        <p>The tradeoff of this reduction in responsibilities is a loss of control over the choices made at each level
          of abstraction.</p>

        <p>For example, developers deploying an application to bare metal servers must account for every part of the
          software stack, all the way down to hardware selection. Infrastructure as a Service (IaaS) abstracts away
          hardware concerns, leaving developers responsible for setting up the entire runtime environment including the
          operating system.</p>

        <p>On the other end of the spectrum are solutions like Backend as a Service, which abstract away server-side
          implementation entirely and simply provide an API to interact with the service. At this end, most choices are
          made by the providers of the service - developers have little, if any, control over implementation details.
        </p>

        <p>Function as a Service sits closer to this end of the abstraction spectrum. Most infrastructure details are
          abstracted away. FaaS platforms provide limited options around container resources and runtime environment,
          but control is largely moved from the application developer to the platform.</p>

        <p>For example, AWS Lambda provides native Linux-based runtimes for NodeJS, Python, Ruby, Java, Go, and .NET,
          and enables customization of available RAM up to 3 GB. However, CPU processor and local storage are not
          configurable.</p>

        <p>FaaS is not an ideal choice for developers that require a high level of precision over the configuration of
          infrastructure and application environment.</p>

        <h5>State</h5>

        <p>As previously mentioned, Function as a Service containers are stateless. While some FaaS providers enable a
          small amount of temporary local storage during function execution, this local storage does not persist between
          executions on the same container. In addition, because developers have no insight or control around which
          container is created in response to a given event, there is no way to directly pass state between specific
          FaaS containers.</p>

        <p>We discuss how Bento approached and resolved this limitation later in the case study.</p>

        <h5>Execution Duration</h5>

        <p>FaaS providers set a limit on the duration of function execution. AWS Lambda functions have a maximum
          duration of 15 minutes. Functions that exceed this limit will timeout and return an error.</p>

        <p>For this reason, FaaS architecture is not well suited for processes that may take a long time.</p>

        <h5>Observability and error detection</h5>

        <p>FaaS platforms are relatively new, and as a result, tooling around observability is not yet robust. Amazon
          Web Services provides per-function logging and alerting for AWS Lambda, and similar tools are available across
          major cloud platforms. </p>

        <p>However, FaaS architectures commonly employ several, if not dozens, of different functions that execute
          concurrently and interact with other backend systems. Related logs and alerts tend to be distributed, rather
          than centralized. In this complex environment, discovering the existence and cause of errors remains a slow
          and mostly manual process.</p>

        <p>With an understanding of both the strengths and tradeoffs of a FaaS architecture, let’s take a look at how we
          built Bento.</p>

      </div>
      <div class="chapter">
        <h2 id="building">3 Building Bento</h2>

        <h3>3.1 Design goals</h3>

        <h5>Speed</h5>

        <p>We built Bento with the primary goal of being <strong>fast</strong>. Video transcoding running on a single
          machine is
          typically a slow process, even with optimizations. Bento aims to offer consumer-grade transcoding speed on
          low-cost architecture.</p>

        <h5>Simple to use</h5>

        <p>The world of video transcoding is deep. There are numerous options and adjustments that can be made to videos
          during the transcoding process. However, many individuals and businesses tend to have a simple set of
          requirements around format and resolution. We built Bento to be simple to use. After deployment, uploading and
          transcoding a video is possible with a few clicks.</p>

        <h5>Private and open source</h5>

        <p>Businesses may not want to host their videos on platforms such as YouTube or Facebook for a variety of
          reasons and may prefer not to host them on a video transcoding platform. Bento is open source software
          deployed to your AWS account. Videos are uploaded and transcoded to Amazon S3 buckets. Your files remain on
          your servers.</p>

        <h5>Support small and medium-scale transcoding demands</h5>

        <p>Existing transcoding solutions (in-house transcoding and third party services) provide maximum value for
          large business. They are ideal for companies that deal with a 24/7 stream of video demands.</p>

        <p>However, many small businesses and individuals have video transcoding requirements at a steady, but smaller
          scale. That could mean processing new videos up to dozens of times a day, or hundreds of times throughout a
          week, with dead periods in-between. It is for these groups that Bento is best-suited.</p>

        <h5>Supported inputs and outputs</h5>

        <p>Given our use case, we opted to provide less control over transcoding settings along with fewer input and
          output options. We chose the following inputs and outputs to cover the most common and widely supported video
          formats and resolutions in use today.</p>

        <p>The Bento pipeline currently supports
          <ul>
            <li>Up to 2 hours of 1080p HD input</li>
            <li>Up to 30 minutes of 4K UHD input</li>
            <li>Bento supports any input video that uses the h264 codec. This includes MP4, MOV, MKV, 3GP, and TS</li>
            <li>Bento outputs MP4 video at 4k, 1080p, 720p, and 360p resolution</li>
          </ul>
        </p>

        <h3>3.2 Design tradeoffs</h3>

        <h5>Optimizations for file size and output quality</h5>

        <p>Because Bento is designed to transcode videos exceptionally fast, we do not pursue compression techniques or
          video quality optimizations that come at the expense of speed.</p>

        <h5>Cost for high-volume businesses</h5>


        <!-- SECTION BEING EDITED, REVISIT -->


        <h3>3.3 Tools</h3>
        <p>The FaaS provider that we chose for Bento is AWS Lambda. AWS Lambda is currently the most widely used FaaS
          platform, and provides the highest limits on configuration details like timeout and memory.</p>

        <p>Bento also uses a variety of resources within the AWS ecosystem.</p>

        <ul>
          <li>Amazon S3 is used for storing video files.</li>
          <li>Amazon DynamoDB is used to track state within our pipeline as well as storing information about users'
            videos.</li>
          <li>Amazon EC2 is used to host Bento’s admin dashboard</li>
        </ul>



        <p>Within our Transcoding Lambdas we are using FFmpeg as our transcoding software. FFmpeg is free, open-source
          software that can perform a vast number of operations on media files. We use FFmpeg for the transcoding stage
          and take advantage of a few convenient tools that it provides elsewhere in the pipeline.</p>

        <h3>3.4 Approach</h3>

        <p>Our approach was to send a video file through a pipeline of functions that execute the following steps:</p>

        <ol>
          <li>Divide the video file into <em>n</em> segments.</li>
          <li>Use <em>n</em> number of Lambda functions, working in parallel, to transcode each individual segment to
            the new output format.</li>
          <li>Merge the transcoded segments together into a completed video file.</li>
        </ol>

        <p>The benefits of employing a FaaS architecture stand out in step two. Let’s take a look into how Function as a
          Service facilitates this crucial step.</p>

        <h4>3.4.1 Instant automated scaling</h4>

        <p>Transcoding a video file creates demand on the CPU and memory of the server doing the processing. Larger
          files create larger demand on computing resources.</p>

        <p>In a traditional computing environment, developers must provision computing resources high enough to meet a
          predicted maximum load. This provisioning process must maintain a delicate balance: available resources should
          be high enough that peak capacity is never reached, while ideally not being so high that excess computing
          capacity goes unused.</p>

        <p>Given the extreme variation in the size of video files, effectively provisioning the “right” level of
          computing power for a single server dedicated to video transcoding is difficult. This is exacerbated for
          businesses that have intermittent transcoding needs: there may be periods where a server is completely idle,
          and periods where it is struggling at peak capacity. This is what is referred to as a “bursty” workload. The
          level of load that hits the system varies greatly over time, and is neither constant nor predictable.</p>

        <!-- GET THE BURSTY DIAGRAM -->

        <p>The instant scaling of FaaS is well-suited for bursty workflows as systems can rapidly meet sudden large
          compute demands and scale back down instantaneously.</p>

        <p>For example, when Bento transcodes a video, the file is split into n segments, where n is relative to the
          length of the video. Bento may produce ten to fifteen segments for a one-minute video, while a sixty-minute
          video will produce around six hundred segments. Each segment is sent to a transcoding function.</p>

        <p>With FaaS, transcoding functions are horizontally scaled to process incoming “segment” events. AWS Lambda
          supports up to one thousand concurrent executions by default, and containers usually start up within a few
          seconds. In the context of Bento, this means we can go from a cold start to transcoding up to one thousand
          video segments simultaneously in under twenty seconds. As transcoding functions complete execution, the system
          rapidly scales down back to zero as the containers are destroyed.</p>

        <!-- GET BURSTY LAMBDA HANDLING DIAGRAM -->

        <h4>3.4.2 The power of concurrent processing</h4>

        <p>Bento relies on the ability to execute functions concurrently in order to cut total transcoding time.</p>

        <p>Video transcoding is serial by default. When a video is transcoded, the file is generally processed from the
          beginning to the end. Recall that video files are very large and the transcoding process is very slow.</p>

        <p>As an example, let’s imagine transcoding a large video file from beginning to end. Transcoding the first
          quarter of the video might take roughly thirty minutes, the next quarter would take around the same amount of
          time, and so on. The entire transcoding job for this single video, handled in a linear fashion, would take
          about two hours.</p>

        <div class="img-wrapper linearSlides softened">
          <button id="reset-linear-time" class="bentoButton" disabled> reset </button>
          <button id="frwrd-linear-time" class="bentoButton"> &rarr; </button>
          <img src="images/diagrams/linear-time1.png" alt="text and code cells example" />
          <p>---</p>
        </div>

        <p>Bento divides a single transcoding job into parts that are processed in parallel. In the previous example,
          imagine that we split the video file into quarters. If we transcode them all at once, that would mean: four
          segments, thirty minutes for each segment, transcoded in parallel. Transcoding this way would bring our
          two-hour job down to just thirty minutes.</p>

        <div class="img-wrapper parallelSlides softened">
          <button id="reset-parallel-time" class="bentoButton" disabled> &larr; </button>
          <button id="frwrd-parallel-time" class="bentoButton"> &rarr; </button>
          <img src="images/diagrams/parallel-time1.png" alt="text and code cells example" />
          <p>---</p>
        </div>

        <p>We’ve cut the total transcoding time by 75% in this example
          scenario. However, Bento’s implementation doesn’t divide videos into quarters. In our approach, we decided to
          split video files into segments that are between two and six seconds in duration. This results in dozens,
          sometimes hundreds, of small segments that usually take under a minute to transcode.</p>

        <p>Bento leverages the high concurrency of Function as a Service architecture to transcode these segments in
          parallel. The result is a significantly shorter time spent transcoding.</p>
      </div>
      <div class="chapter">
        <h2 id="results">4 Benchmarking Bento</h2>

        <p>We benchmarked Bento against two options that individuals and small businesses might choose for video
          transcoding. </p>

        <!-- GET COMPETITOR DIAGRAM -->

        <p>The first alternative is a free-tier EC2 instance with 1 GB of RAM, running Ffmpeg. While this is a barebones
          option for professionals, it represents a clear baseline to benchmark Bento’s performance against traditional,
          non-optimized video encoding on a single machine.</p>

        <p>For the second alternative, we chose Amazon Elemental MediaConvert. MediaConvert is a feature-rich
          transcoding service, aimed at professional broadcasters and media companies. While Bento does not provide the
          same set of transcoding options, we wanted to demonstrate the speed Bento can achieve against
          professional-grade software.</p>

        <p>As a test, we measured the time it took Bento and these two alternatives to output a video file to MP4 format
          at 1280x720 resolution.</p>

        <p>We benchmarked against nineteen test videos, whose size ranged from four megabytes to two gigabytes, and
          whose duration spanned seven seconds to ninety minutes.</p>

        <p>In the chart below, we provide a cross-section of benchmark data that illustrates the results we saw at small
          (&lt; 100MB), medium (100+ MB), large (1+ GB) and very large (2+ GB) file sizes.</p>

        <h3>4.1 Benchmarking Results</h3>

        <!-- GET RESULTS DIAGRAM -->

        <p>Our benchmarks demonstrate that a serverless, massively parallel approach to video encoding results in
          significant improvements to transcoding speed.</p>

        <p>Bento consistently performs over 90% faster than an EC2 instance in our benchmarks, and 50% faster than
          Amazon’s MediaConvert transcoding service across a variety of file sizes, video durations, and format types.
        </p>

        <p>Let’s dive into Bento’s architecture and explore how Bento achieves these results.</p>
      </div>
      <div class="chapter">
        <h2 id="architecture">5 Bento Architecture</h2>

        <p>The Bento pipeline consists of four stages, organized in a fan-out fan-in pattern. In a fan-out fan-in
          pattern, a single large task is split into multiple smaller subtasks that are run in parallel (fan-out).
          The
          results of those individual subtasks are then aggregated and processed by a single reducing function
          (fan-in).
        </p>

        <div class="img-wrapper">
          <img src="images/gifs/overview.gif" alt="Overview of Bento Architecture" id="session-data-screenshot" />
          <p>---</p>
        </div>

        <p>Bento’s four stages are:</p>

        <h5><strong>Stage 1 - Executor</strong></h5>
        <p>The Executor function determines the start and end times of each video segment and invokes an instance of the
          Transcoder function for each one of these segments.</p>

        <h5><strong>Stage 2 - Transcoder</strong></h5>
        <p>The Transcoder function transcodes a small segment of the video file and saves the segment to intermediate S3
          storage. Multiple instances of the Transcoder function run at this stage - one for each segment.</p>

        <h5><strong>Stage 3 - Merge Invoker</strong></h5>
        <p>The Merge Invoker function checks the state of the pipeline to determine when all segments have been
          transcoded. At that point, it invokes the Merge function.</p>

        <h5><strong>Stage 4 - Merger</strong></h5>
        <p>The Merge function concatenates the transcoded video segments into a complete, transcoded video.</p>

        <p>Let’s explore how we built each stage in more depth.</p>

        <h3>5.1 Stage 1 - Executor</h3>

        <h5>Goals:</h5>

        <ul>
          <li>Split video into n segments</li>
          <li>Invoke the Transcoder for each segment</li>
        </ul>

        <div class="img-wrapper">
          <img src="images/gifs/executor.gif" alt="Executor diagram" id="session-data-screenshot" class="softened" />
          <p>---</p>
        </div>

        <h4>5.1.1 Segmenting a video file</h4>

        <p>To begin the process, the Executor receives an invocation event triggered by Bento’s admin dashboard. The
          body of the request has information about which video to transcode and what the resolution of the transcoded
          video should be.</p>

        <p>Our first concern is splitting an original video file into segments that can be passed to instances of a
          transcoder function.</p>

        <p>Our initial approach was to use Ffmpeg’s segment muxer. This tool divides an input video into segments of
          m-second length. As a starting point, we chose segments that were four seconds in length.</p>

        <p>At this point, we invoke the Transcoder function for each video segment. When FaaS functions are invoked,
          they can be passed JSON data in the body of the request. Because we can’t directly pass video files between
          functions, we use Amazon S3 as intermediate storage for video segments.</p>

        <p>We save each video segment to S3, then pass the location of a video segment in S3 to each invocation of the
          transcoder function.</p>

        <p>Although this approach achieves our objective, we noticed early on that the Executor function was taking much
          longer than we expected to complete. Video files that were a few hundred megabytes were hitting our function’s
          execution time limit of 15 minutes.</p>

        <p>As it turns out, Ffmpeg’s segment muxer was transcoding the entire video file as it was splitting it into
          segments. At this point, we were encountering the exact challenge that Bento aims to solve - slow video
          transcoding on single machines.</p>

        <h4>5.1.2 Mapping a video for segment times</h4>

        <p>We decided on an alternate approach. Rather than splitting a video file into segments, we use the Executor to
          map out the start and end times of each segment we want to create.</p>

        <p>We can then have each instance of the Transcoder function work with the full video file, but only transcode
          within the bounds of the times that we pass to it.</p>

        <p>Within the Executor, we use another Ffmpeg utility, Ffprobe, to examine the video and collect metadata about
          the video’s keyframes and timestamps. The Executor function uses this information to determine the start and
          end times of each segment.</p>

        <p>We save information about each segment to a table in DynamoDB, and save the status of the entire transcoding
          job to a separate table. In section 5.1 of this case study, we will explore the challenges of preserving state
          across Function as a Service in more detail.</p>

        <p>Once the Executor maps out segment data, it finishes by invoking an instance of the Transcoder function for
          each segment. It passes along the desired transcoding settings, and the start and end times that the instance
          is responsible for processing.</p>

        <!-- GET PAYLOAD DIAGRAM -->

        <h3>5.2 Stage 2 - Transcoder</h3>

        <h5>Goals:</h5>

        <ul>
          <li>Transcode a segment of the video file</li>
        </ul>

        <div class="img-wrapper">
          <img src="images/gifs/transcoder.gif" alt="Transcoder diagram" id="session-data-screenshot"
            class="softened" />
          <p>---</p>
        </div>

        <p>The Transcoder function receives information about the transcoding job, including the timestamps that mark
          the beginning and end of a segment in the original video.</p>

        <p>Because AWS Lambda functions have limited temporary local storage, we quickly realized that we needed to
          transcode the original video file without saving it to disk. Fortunately, Ffmpeg accepts input over HTTP,
          allowing us to process the video in memory and bypass the need to store the full file locally. We expand on
          the challenges of storing and processing large video files in a FaaS architecture in section 5.2 of this case
          study.</p>

        <p>To transcode a segment of the video, we pass the URL of the original video file to Ffmpeg, along with the
          desired transcoding settings, and the start and end times of the segment. Ffmpeg uses the h264 codec to
          transcode the video segment and packages it in the MP4 container.</p>

        <p>The result of this transformation, an m-second video in MP4 format, is saved to temporary local storage. The
          function uploads this newly created segment to an intermediate S3 bucket for persistent storage.</p>

        <p>After uploading the segment to S3, the Transcoder updates the records for the segment and job in their
          respective DynamoDB tables, marking an additional segment as completed. </p>

        <p>Before we move on, it's important to note that although we are zoomed in to look at a single instance of the
          Transcoder function, there may be hundreds of Transcoder functions executing in parallel, each working on a
          different segment of the original video.</p>

        <p>As each instance of the Transcoder function finishes transcoding its respective segment, we need to carefully
          consider how - and crucially, <em>when</em> - to concatenate the segments into a complete transcoded video.
        </p>

        <h3>5.3 Stage 3 - Merge Invocation</h3>

        <p>Goals:</p>

        <ul>
          <li>Invoke the Merge function only once all segments have been transcoded</li>
        </ul>

        <h4>5.3.1 Coordinating pipeline stages amidst concurrency</h4>

        <div class="img-wrapper">
          <img src="images/gifs/mergeInvoke.gif" alt="Merge Invoke diagram" id="session-data-screenshot"
            class="softened" />
          <p>---</p>
        </div>

        <p>When we reach the end of the Transcoding stage, we require some sort of event to indicate that we are ready
          to move to the Merge stage. Generating this event becomes a significant challenge, as the stateless nature of
          FaaS containers and the unpredictable timing of concurrently executing functions prevents a straightforward
          approach to coordinating the final step. These challenges are explored in detail in section 5.1.</p>

        <p>The Merge Invocation stage encapsulates the solution to this problem. The objective of this stage is to
          invoke the Merge function only when all the segments of a video have been successfully transcoded.</p>

        <h4>5.3.2 Capturing change in state with DynamoDB Streams</h4>

        <p>To implement the Merge Invocation stage, we rely on a feature of DynamoDB, called DynamoDB Streams. DynamoDB
          Streams are time-ordered series of records that are created when a given database table is updated. Each
          record encapsulates a change that was made to an item in the table. Crucially, Lambda functions can be
          triggered when new records are added to the stream. This behavior forms the basis of the Merge Invoker.</p>

        <p>First, we attach a DynamoDB Stream belonging to the Jobs table to the Merge Invoker function.</p>

        <p>A record in the Jobs table maintains a count of the total number of segments to be transcoded for that job,
          and the number of completed segments that have been successfully transcoded. When a segment has been
          transcoded, the Transcoder function updates the associated job record, incrementing the number of completed
          segments by one.</p>

        <p>The update causes the Jobs table to emit a record into the event stream, which in turn invokes the Merge
          Invoker function.</p>

        <h4>5.3.3 Invoking the Merge</h4>

        <p>The Merge Invoker receives the event from the stream, which contains the latest snapshot of the job record.
          The function compares the number of completed segments to the total number of segments to determine whether
          all the segments for this particular job have been successfully transcoded.</p>

        <p>If all the segments are transcoded, the Merge Invoker will invoke the Merge function.</p>

        <h3>5.4 Stage 4 - Merge</h3>

        <h5>Goals:</h5>

        <ul>
          <li>Concatenate the transcoded segments into one complete video</li>
        </ul>

        <p>When the Merge function is invoked, all transcoded segments are available in intermediate S3 storage to be
          merged into the final file. To merge these videos together, we use the ffconcat utility provided by Ffmpeg,
          which concatenates multiple input files into a single output file.</p>

        <div class="img-wrapper">
          <img src="images/gifs/merge.gif" alt="Merge diagram" id="session-data-screenshot" class="softened" />
          <p>---</p>
        </div>

        <p>As in previous stages, our intention was to perform this processing without saving the video segments, or the
          complete final video, to temporary local storage. As before, we employed Ffmpeg’s ability to process input via
          HTTP to concatenate the video segments in memory.</p>

        <p>To avoid storing the final file on disk, we piped Ffmpeg’s byte stream output directly to S3 storage. We
          expand on the challenges of streaming this video data directly to remote storage in section 5.2</p>

        <p>Once the completed video file has been saved to S3, we update our record in the Jobs table to indicate that
          the transcoding job has been completed. With that, the job is complete.</p>
      </div>
      <div class="chapter">
        <h2 id="challenges">6 Challenges</h2>

        <p>We faced two main problem domains while building Bento: 1) dealing with concurrency in the context of Event
          Driven Architecture and 2) limited function storage.</p>

        <h3>6.1 Pursuing an Event Driven Architecture (EDA)</h3>
        <h4>6.1.1 What is Event Driven Architecture?</h4>

        <p>Function as a Service is event-driven. Therefore, building an application with Function as a Service implies
          the use of an Event Driven Architecture.</p>

        <p>In section 4 of this case study, we refer to specific Lambda functions being “invoked” by other functions.
          This is a reasonable, but incomplete mental model for the interaction that takes place.</p>

        <p>When a function in the Bento pipeline is “invoked”, it is responding to an invocation event that is placed in
          a messaging queue. Invocation messages can be placed in a Lambda function’s queue as a response to an AWS API
          request - for instance, when the Executor “invokes” the Transcoder function. Invocation messages can also be
          added in response to changes in state in other backend systems - for instance, a record emitted by DynamoDB
          Stream is enqueued by the Merge Invoker function. Lambda functions spin up to consume all the available
          messages placed in their queue until there are no more messages remaining in the queue, or a concurrency limit
          is reached.</p>

        <p>Event Driven Architecture stands in contrast to more tightly coupled procedural programming patterns like
          REST. Interactions occur asynchronously, and event producers are decoupled from event consumers. One of the
          features this facilitates is the rapid scaling FaaS is able to accomplish in response to incoming events.</p>

        <h4>6.1.2 Problem: Tracking the sequence of concurrent events</h4>

        <p>Purely event-driven applications react to internal and external events without a central orchestrator [1].
          Instead, execution flow is completely determined by the occurrence of individual events.</p>

        <p>However... this is not guaranteed In this case if the fourth Transcoder was predicted to finish last the
          merge function would have been invoked prematurely, before all the segments were available Since we could
          not predict which function would complete last we considered moving away... from a strict serverless EDA
          model Our goal was now to capture state that functions could read throughout the pipeline with a DB On to
          a
          second problem related to concurrency By introducing a DB to our pipeline we discovered first hand that
          concurrent processes interacting with a DB is a recipe for race conditions A DB allowed us to store
          metadata
          to track state and metadata to track our pipeline’s performance To capture the state of our pipeline
          additional logic was added to transcoders After transcoding a segment each function now: Updates a
          segment’s
          status from pending to complete in the Segments table Then Updates the total number of transcoded segments
          that are complete for a job in the Jobs table Our DB reliably stores state, good, but the DB cannot
          orchestrate the transition from the transcoding stage to the merging stage on its own We tested a model
          where any Transcoding function could orchestrate the merge phase Before winding down these functions would
          read the Jobs table - check if all the segments were transcoded and if so trigger the merge phase This
          approach was not reliable because a race condition was introduced.
          A function writes to the Jobs table, incrementing it’s completed segments counter A concurrently running
          function completes and also increments the counter The previous function performs its read of the counter
          and sees that all the segments are available the last function to complete performs its read and also sees
          that all the segments are available The problem: this results in multiple invocations of the merge
          function,
          not what we want Even with a DB to update and read state the responsibility of orchestrating the Merge
          stage
          was not suitable for a Transcoder Our solution... capturing changes to the DB as events We needed a way to
          receive and process updates to the Jobs table sequentially The creation of an event stream attached to the
          Jobs table provides serialization of any updates within the table Capturing a time-ordered sequence of
          updates to a table is referred to as change-data-capture A resource was now required to examine the
          records
          of this stream Back to the function, Invoke Merge this function was attached to the stream: and is invoked
          when new stream records are detected with each record Invoke Merge checks the counter attribute of the Job
          to table see if all the segments are ready, when they are the Merge phase is triggered ...While pursuing
          an
          Event Driven Architecture we encountered common problems related to concurrency: it is difficult to track
          the state of concurrently running processes and race conditions are common. A DB proved to be a
          lightweight
          addition to the pipeline and gives us the option to the track outcomes related to our pipelines
          performance.
          Let’s move to our second problem domain: challenges related to storage
        </p>

        <h3>6.2 Limited function storage</h3>

        <p>Video files are LARGE, if our pipeline can only handle small files our use case will be severely
          restricted
          Let’s take a look at how Bento went from processing videos around 250MB to 2 GB! On AWS, serverless
          functions, Lambdas, are containerized Containers hold resources that are allocated to each Lambda Each
          Lambda is allocated 512MB of temporary storage Our starting point was a pipeline restricted to video files
          of no more than ~250MB, this was because of the way we were managing temporary storage The main culprit
          was
          the Merge stage The Merge function retrieves processed video chunks … and assembles them into our output
          video To do all this we first had the merge function download all of the segments to its temporary storage
          ffmpeg, our video transcoding software, was then able to concatenate these segments The Problem: Both the
          segments and the final video were living in temporary storage before the final video could be shipped off
          to
          S3, our permanent storage Bento could only handle videos up to roughly 250MB Our solution to this problem:
          avoid storing the segments in /tmp storage and instead have them processed in memory Ffmpeg does not rely
          on
          locally stored videos, it also accepts http input This means Segments can be downloaded and concatenated
          in
          memory Now..only the output is taking up temporary storage This is helpful because this function has 3GB
          of
          memory and only ½ GB of storage Our file size capacity doubled! To 500MB But… in the world of video 500MB
          isn’t all that much Our input, the segments, could actually total much more than 500MB because the Merger
          function has 3GB of memory to hold and process the segments Our storage problem now related to output
          Let’s
          take a closer at this output bottleneck: how the entire final video gets stuck in /tmp storage ffmpeg
          gradually builds up the output in storage Once ffmpeg finishes the concatenation process and the entire
          video is sitting in the Lambdas storage We then move the file to S3 Our pipeline was restricted to files
          ~500MB, the output of the Merger function had to be optimized to make more breakthroughs Our goal was to
          have the final file built up on S3 instead of first having to live in /tmp storage We discovered that
          ffmpeg
          output can be piped as a stream Instead of holding the final output it could be sent in pieces as a byte
          stream directly to S3 AWS S3 storage supports http Multipart uploads Multipart uploads allow a file to be
          assembled in parts .. even if the parts are not sent in order our merged video can now be gradually
          constructed in our permanent storage, bypassing /tmp storage Here’s a bird’s eye view of present day
          Merger
          The transcoded segments are processed by ffmpeg in memory thanks to http input and the final video is
          built
          directly on S3 via http Multipart upload Because our function has 3GB of memory our pipeline can now
          handle
          videos upwards of 2GB We improved the reliability of our pipeline by injecting state and increased our
          ability to handle large files </p>

        <p>Now that we’ve talked about Bento’s architecture, let’s dive into Bento’s performance. </p>
      </div>
      <div class="chapter">
        <h2 id="results">7 Bento Dashboard</h2>

        <p>To easily transcode videos with Bento, we’ve provided a user interface that we call <strong>Bento
            Dashboard</strong>. Bento
          Dashboard is built in React with an Express backend that can be run locally for individual use, or deployed to
          Amazon EC2 for organizational use.</p>

        <p>Bento Dashboard enables users to upload locally stored videos to S3 for transcoding. Once uploaded,
          transcoding jobs can be run with a few clicks.</p>
      </div>
      <div class="chapter">
        <h2 id="future-work">8 Future work</h2>

        <p>We’d like to highlight some opportunities to enhance Bento that we’re excited about.</p>

        <h5>Support for HLS and DASH output</h5>

        <p>Bento currently supports MP4 output because it is the most widely used and supported video file format in use
          today. We’d like to add support for HLS and DASH formats, which together represent the most common adaptive
          bitrate streaming formats. Adaptive bitrate streaming is a technique employed by modern video players that
          enable on-the-fly quality adjustments to changes in available bandwidth.</p>

        <h5>Support for additional video transformation</h5>

        <p>Many businesses perform additional transformations to their videos, including adding subtitles and
          intro/outro bumpers. Bento’s transcoding pipeline is well-suited to be extended to perform these sorts of
          transformations.</p>
      </div>
  </main>

  <section id="our-team">
    <h2 id="team">Our Team</h2>

    <p>
      We are looking forward to new opportunities. Please contact us if this project interested you, or if you have
      any questions!
    </p>

    <ul>
      <li class="individual">
        <img src="images/avatars/mike.png" alt="Mike Del Rio" />

        <h3>Mike Del Rio</h3>

        <p>Toronto, ON</p>

        <ul class="social-icons">
          <li>
            <a href="#" target="">
              <img src="images/icons/email_icon.png" alt="email" />
            </a>
          </li>

          <li>
            <a href="#" target="_blank">
              <img src="images/icons/website_icon.png" alt="website" />
            </a>
          </li>

          <li>
            <a href="#" target="_blank">
              <img src="images/icons/linked_in_icon.png" alt="linkedin" />
            </a>
          </li>
        </ul>
      </li>

      <li class="individual">
        <img src="images/avatars/max.png" alt="Max Hawkins" />

        <h3>Max Hawkins</h3>

        <p>Alameda, CA</p>

        <ul class="social-icons">
          <li>
            <a href="#" target="">
              <img src="images/icons/email_icon.png" alt="email" />
            </a>
          </li>

          <li>
            <a href="#" target="_blank">
              <img src="images/icons/website_icon.png" alt="website" />
            </a>
          </li>

          <li>
            <a href="#" target="_blank">
              <img src="images/icons/linked_in_icon.png" alt="linkedin" />
            </a>
          </li>
        </ul>
      </li>

      <li class="individual">
        <img src="images/avatars/nathan.png" alt="Nathan Classen" />

        <h3>Nathan Classen</h3>

        <p>Tulsa, OK</p>

        <ul class="social-icons">
          <li>
            <a href="#" target="">
              <img src="images/icons/email_icon.png" alt="email" />
            </a>
          </li>

          <li>
            <a href="#" target="_blank">
              <img src="images/icons/website_icon.png" alt="website" />
            </a>
          </li>

          <li>
            <a href="#" target="_blank">
              <img src="images/icons/linked_in_icon.png" alt="linkedin" />
            </a>
          </li>
        </ul>
      </li>
    </ul>
  </section>

  <section id="footnotes">
    <h2 id="references">References</h2>
    <h4 id="ref-heading">Event Driven Architecture</h4>
    <ul>
      <li>
        <a href="https://towardsdatascience.com/jupyter-lab-evolution-of-the-jupyter-notebook-5297cacde6b"
          target="_blank">Jupyter Lab: Evolution of the Jupyter Notebook</a>
      </li>
      <li>
        <a href="https://blog.nteract.io/nteract-building-on-top-of-jupyter-9cfbccdd4c1d" target="_blank">nteract:
          Building on Top of Jupyter</a>
      </li>
      <li>
        <a href="https://moderndata.plot.ly/nteract-revolutionizing-notebook-experience/" target="_blank">nteract:
          Revolutionizing the Notebook Experience</a>
      </li>
      <li>
        <a href="https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/"
          target="_blank">The Scientific Paper is Obsolete</a>
      </li>
      <li>
        <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233" target="_blank">Notebook
          Innovation at Netflix</a>
      </li>
      <li>
        <a href="https://hackernoon.com/simplify-devops-with-jupyter-notebook-c700fb6b503c" target="_blank">Devops
          with Juypter</a>
      </li>
      <li>
        <a href="https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g37ce315c78_0_27"
          target="_blank">I Don't Like Notebooks</a>
      </li>
      <li>
        <a href="https://yihui.name/en/2018/09/notebook-war/" target="_blank">Notebook Wars</a>
      </li>
      <li>
        <a href="https://blog.runkit.com/2015/09/10/time-traveling-in-node-js-notebooks/" target="_blank">Time
          Traveling in Node.js Notebooks</a>
      </li>
      <li>
        <a href="https://medium.com/netflix-techblog/open-sourcing-polynote-an-ide-inspired-polyglot-notebook-7f929d3f447"
          target="_blank">Open-Sourcing Polynote</a>
      </li>
      <li>
        <a href="https://observablehq.com/@observablehq/observables-not-javascript%3E" target="_blank">Observable's
          Not Javascript</a>
      </li>
    </ul>
  </section>
</body>

</html>